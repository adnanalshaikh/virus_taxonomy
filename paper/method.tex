

\section {Method and material}

We employed a hierarchical learning approach to classify the taxonomy of a virus based on its DNA sequence. Our method begins with an initial classifier that predicts the order of the virus. Once the order is identified, a second classifier is used to determine the family. Although further classifiers could be applied to predict the genus and species of the virus, this research focuses primarily on evaluating the effectiveness of the classifier. Therefore, we limited our study to the two viral orders, Martellivirales and Tymovirales as shown in Figure{fig:taxonomy} and Table{table:species:counts}.

After the order is predicted, the corresponding family classifier is applied to identify the specific family of the virus within that order. To maintain robust classification performance, we excluded families with fewer than five species and species with sequence lengths shorter than 100 base pairs.

This hierarchical approach enables accurate virus classification from DNA sequences, offering valuable insights into the virus’s characteristics and potential impact. By dividing the classification process into smaller, targeted steps, we effectively manage the complexity of viral taxonomy, enhancing our capacity to study and address viral threats. Figure~{fig:architecture} shows the a block diagram of the computational environment that we adapted in this research. In the following subsections, we discuss the dataset used, data preprocessing, and the machine learning model applied in this study.

\begin{figure}[t]
\begin{centering}
		\includegraphics[width=.5\textwidth]{Figures/hierarchy2.png}
	\caption{distribution  }
\label{fig:taxonomy}
\end{centering}
\end{figure}


\begin{figure*}[t]
\begin{centering}
		\includegraphics[width=\textwidth]{Figures/archit.png}
	\caption{distribution  }
\label{fig:architecture}
\end{centering}
\end{figure*}

\begin{table}[tb]
\centering
\begin{tabular}{| l | l | l |}
\hline
\textbf{Order} & \textbf{Family} & \textbf{Species} \\
\hline
\multirow{5}{*}{Martellivirales} & Virgaviridae & 56 \\
                                  & Closteroviridae & 54 \\
                                  & Bromoviridae & 38 \\
                                  & Togaviridae & 37 \\
                                  & Endornaviridae & 31 \\
\hline
\multirow{3}{*}{Tymovirales}     & Betaflexiviridae & 125 \\
                                  & Alphaflexiviridae & 65 \\
                                  & Tymoviridae & 34 \\
\hline
\end{tabular}
\caption{Number of families and species in each order.}
\label{table:species:counts}
\end{table}


\subsection{Dataset}
\label{sec:obj:det}

The dataset for this research was sourced from the National Center for Biotechnology Information (NCBI) RefSeq database, which offers high-quality, curated sequences that have been reviewed and annotated by experts, ensuring accuracy and reliability. RefSeq provides a single, well-characterized reference sequence for each virus, minimizing redundancy and simplifying data interpretation. Additionally, each RefSeq entry has a stable identifier, facilitating consistent referencing and reporting.

To acquire the relevant sequences, we used the latest Virus Metadata Resource (VMR) spreadsheet available from the International Committee on Taxonomy of Viruses (ICTV) at https://ictv.global/vmr/current, as well as on the associated GitHub repository. This file contains comprehensive metadata for all viruses. Using RefSeq accession numbers listed in the VMR spreadsheet, we downloaded the DNA sequences for the taxa of interest in this study.

In our research, we focused on the classification of viruses within the Alsuviricetes class, belonging to the Kitrinoviricota phylum, which is part of the Orthornavirae kingdom in the Orthornavirae realm. We developed a hierarchical classification approach, starting by categorizing viruses into two orders: Martellivirales and Tymovirales.

Subsequently, we employed specialized classifiers to distinguish among the families within each order. For the Martellivirales order, we differentiated between the families Virgaviridae, Closteroviridae, Bromoviridae, Togaviridae, Endornaviridae, and Solspiviridae. For the Tymovirales order, we further classified viruses into Betaflexiviridae, Alphaflexiviridae, and Tymoviridae families. This multi-level classification approach allowed us to systematically analyze and categorize the viral sequences with a high degree of precision. Table \ref{table:species_counts} provides an overview of the number of families and species in each order. This structured dataset enabled us to perform a comprehensive and precise classification of viral sequences across the chosen taxonomic levels.

\subsection{Data Preprocessing} 

To prepare our dataset for effective analysis, we implemented several preprocessing steps aimed at improving data quality and ensuring consistency:

\begin{enumerate}
\item{Data Filtering:}
We began by filtering out viral families that had fewer than five representative sequences and excluded any sequences shorter than 100 nucleotides. This step reduced noise and ensured that our dataset had sufficient representation across various families, as shown in Figure 3. Each sequence was identified with an unique id.

\item{Handling Ambiguous Nucleotides:}
Ambiguous characters within DNA sequences, such as ‘M’ or ‘N,’ were replaced with the most frequent nucleotide in each sequence. For DNA sequences, unidentified nucleotides were substituted as follows:

\begin{itemize}
\item{DNA Substitution:} Non-‘A,’ ‘T,’ ‘G,’ or ‘C’ characters were replaced with the most frequent nucleotide within the sequence.
\item{RNA Substitution:} Non-‘A,’ ‘U,’ ‘G,’ or ‘C’ characters were replaced similarly for RNA sequences. This approach minimized the variability introduced by ambiguous nucleotides and enhanced the uniformity of the dataset.
\end{itemize}

\item{Data Augmentation:}
To increase the size of our training dataset and enhance model generalizability, we applied two data augmentation techniques:
\begin{itemize}
\item{Reverse Complement Generation:}
We generated the reverse complement for each DNA sequence. This involves reversing the order of nucleotides and substituting each with its complementary base pair (e.g., ‘A’ with ‘T,’ ‘C’ with ‘G’). This technique doubled the size of our dataset while preserving the biological integrity of the sequences, allowing the model to recognize patterns independent of sequence orientation. Each reverse complement sequence was identified by a aunique sequence number and labeled as a reverse complement of the original sequence.

\item{Sliding Window Splitting:}
Each sequence (including its reverse complement) was divided into fixed-length subsequences using a sliding window approach. We dynamically adjusted the stride for each sequence to ensure a consistent number of subsequences, calculated using the formula:

$$  s = \frac{L-n} {m-1},$$

 
where $L$ is the total length of the sequence, $n$ is the length of each subsequence, 
$m$ is the number of subsequences to generate, and $s$ is the step size for the sliding window, which determines how far the window moves between generating subsequences. For example, to generate 3 subsequences, each of length 100, from a sequence of length 300, the stride would be 100. In contrast, to generate 3 subsequences of length 100 from a sequence of length 200, the stride would be 50.
All subsequences of the same sequence have the same id as their sequence.

This method ensured uniform input dimensions, reduced the risk of overfitting, and enabled the model to learn patterns effectively across the dataset.
\end{itemize}

\item{One-Hot Encoding:}
We used one-hot encoding to convert the DNA sequences into a numerical format suitable for our machine learning models. Unlike k-mer encoding, which abstracts sequence information, one-hot encoding preserves the position of each nucleotide, allowing our models (1D CNN and LSTM) to capture critical sequential and spatial patterns.

\item{Dataset Splitting:}
To evaluate our model effectively, we split the dataset into three subsets: the training set, the validation set, and the test set. The validation set was used to tune the model's hyperparameters, while the test set was reserved for the final evaluation of the model's performance. The splitting was carefully based on the sequence ID to prevent data leakage. Specifically, all subsequences of a given sequence ID were assigned entirely to the test set, ensuring that no subsequences or their reverse complement subsequences were present in the training or validation sets. Similarly, all subsequences of a sequence were grouped within the validation set, ensuring that neither the original subsequences nor their reverse complements appeared in the training or test sets. This rigorous splitting strategy guaranteed that no data leakage occurred, preserving the integrity of the evaluation.



\end{enumerate}

By applying these preprocessing steps, we enhanced the quality and consistency of our input data, ensuring that our models could learn meaningful features from a well-structured and augmented dataset.

\begin{figure*}[t]
\begin{centering}
		\includegraphics[width=\textwidth]{Figures/cnn-archit.png}
	\caption{distribution  }
\label{fig:cnn-archit}
\end{centering}
\end{figure*}

\subsection{Hierarchical Classification}

The ICTV scheme organizes classes into a hierarchical taxonomic tree. From the highest to the deepest levels, these are: Realm, Kingdom, Phylum, Subphylum, Class, Order, Family, Subfamily, Genus, and Species. In hierarchical classification, our aim is to predict a set of hierarchically structured classes for each virus. A comprehensive review of hierarchical classification is available in [81][82]. Hierarchical classifiers can be categorized into three types based on how they utilize hierarchical information: the flat classifier approach, the local classifier approach, and the global classifier (big-bang) strategy.

The Flat Classifier Approach is the most straightforward method for hierarchical classification. It involves using a standard multi-class classifier to make predictions at the lowest level of the hierarchy. Subsequently, a post-processing phase is applied to assign higher-level labels based on these predictions.

The Local Classifier Approach explicitly incorporates the class hierarchy into the categorization process. This technique utilizes a combination of multi-class non-hierarchical classifiers, often referred to as base classifiers, which operate at different levels or nodes of the hierarchy. Each classifier is responsible for distinguishing between immediate child classes of a given parent class, thus leveraging the hierarchical structure during classification. There are three types of local hierarchical classification, depending on what data the classifier is trained on. The inference process is essentially the same across these approaches:
\begin{enumerate}
\item{Local Classifiers per Node (LCN)}: A classifier is trained for each internal node in the hierarchy to distinguish between its child nodes. When classifying a new instance, the system traverses the hierarchy from the root, making decisions at each node until it reaches a leaf node. For example, in virus taxonomy, a classifier at the "Order" level decides between different orders, and for each order, another classifier distinguishes between families within that order.
\item{Local Classifiers per Parent Node (LCPN)}: Similar to LCN, but instead of having a classifier for each node, there is one classifier for each parent node, distinguishing among its children. This approach reduces the number of classifiers compared to LCN and is used similarly by traversing the hierarchy from the root. For instance, a classifier at the "Order" level decides among orders, and if it chooses "Norzivirales," a classifier specific to "Norzivirales" determines the family. In this paper, we adapted this approach.
\item{Local Classifiers per Level (LCL)}: A single classifier is trained for each level of the hierarchy. For a given instance, the classifier for the first level determines the category, and this process is repeated at each subsequent level using the corresponding classifier. For example, one classifier determines the order, another determines the family based on the predicted order, and so on.
\end{enumerate}

The Global Classifier Approach (Big-Bang Strategy) defines a single optimization problem that considers the entire class hierarchy simultaneously. Instead of handling hierarchical information at different levels or nodes independently, this strategy integrates the hierarchical structure into a unified model, aiming to optimize the classification across all levels of the hierarchy in one step.

In this paper, we adopted the Local Classifiers per Parent Node (LCPN) approach. This approach reduces the number of classifiers compared to the Local Classifiers per Node (LCN) method, while still capturing hierarchical information. Unlike the flat classifier approach, LCPN explicitly incorporates the class hierarchy. Moreover, it is more intuitive and simpler than the big-bang classification strategy.

Since the focus of this research is on the LCPN approach, we used one classifier to distinguish the order and two classifiers to predict the family as shown in figure 1.

%The ICTV scheme organizes classes into a hierarchical taxonomic tree. From the highest to the deepest levels, these are: Realm, Kingdom, Phylum, Subphylum, Class, Order, Family, Subfamily, Genus, and Species. In hierarchical classification, our aim is to predict a set of hierarchically structured classes for each virus. A comprehensive review of hierarchical classification is available in [81][82]. Hierarchical classifiers can be categorized into three types based on how they utilize hierarchical information: the flat classifier approach, the local classifier approach, and the global classifier (big-bang) strategy. In this paper, we adopted the Local Classifiers per Parent Node (LCPN) approach. This approach reduces the number of classifiers compared to the Local Classifiers per Node (LCN) method, while still capturing hierarchical information. Unlike the flat classifier approach, LCPN explicitly incorporates the class hierarchy. Moreover, it is more intuitive and simpler than the big-bang classification strategy.

\subsection{Classification Method}

\begin{figure*}[t]
\begin{centering}
		\includegraphics[width=\textwidth]{Figures/acc_loss_3taxa.png}
	\caption{distribution  }
\label{fig:cnn-archit}
\end{centering}
\end{figure*}

In this paper, we implemented the Local Classifiers per Parent Node (LCPN) approach using a deep learning architecture. Specifically, we employed a one-dimensional Convolutional Neural Network (1D CNN) to extract essential features from DNA sequences. These features were then passed to a Long Short-Term Memory (LSTM) layer, which captured sequential dependencies and patterns in the DNA data. The output from the LSTM layer was subsequently fed into a dense (fully connected) layer, which was trained to accurately predict the class of the viral sequence.

Our deep learning model architecture consists of four 1D convolutional layers, with an increasing number of filters as the network deepens. The first two convolutional layers have 64 filters each, with a filter size of 3. The next two convolutional layers have 128 filters each, also with a filter size of 3. Each 1D convolutional layer is followed by a Batch Normalization layer, which is then followed by an Activation layer using the ReLU activation function. All layers, except the first convolutional layer, are additionally followed by an Average Pooling layer to reduce the spatial dimensions.

The 1D CNN is followed by a single LSTM layer with 256 units, which captures long-term dependencies in the DNA sequences. The LSTM layer is followed by a Batch Normalization layer to stabilize and accelerate training. Next, a Flatten layer is used to convert the output into a format suitable for the dense layers. The Flatten layer is followed by two Dense layers, each with 256 units. Each Dense layer is accompanied by a Batch Normalization layer, an Activation layer using the ReLU function, and a Dropout layer to prevent overfitting. Finally, the output layer uses the softmax activation function to produce class probabilities.
  
\subsection{Model Training and Inference:}
At each node in the hierarchy, we trained the model using all subsequences from the training dataset. The validation score was then calculated using all subsequences from the validation dataset to fine-tune the model. During inference, we utilized the test dataset, where a prediction probability was generated for each subsequence of a given sequence. We then aggregated the predictions of all subsequences using soft voting to obtain the final prediction for the entire sequence. This approach ensured that the model's output for each sequence was robust and accounted for variations across its subsequences



\subsection{Performance Measures:}
We evaluated our models using a cross-validation approach at each node in the hierarchical classification framework. Standard performance metrics, including accuracy, precision, recall, and F1-score, were employed. Given the multiclass nature of our classification problem, we applied micro-averaging to aggregate the results, ensuring a balanced evaluation across all classes.

For hierarchical evaluation, we extended these measures to account for the hierarchical structure of the classification task. Specifically, we assessed the performance based on the full path accuracy, which considers whether the model correctly predicted the entire sequence of class labels from the root to the leaf node. Additionally, we computed hierarchical precision, recall, and F1-score by evaluating whether predictions were accurate at each level of the hierarchy. This comprehensive approach allowed us to measure not only the performance of individual classifiers at each node but also the overall effectiveness of the hierarchical classification system.

By combining traditional performance metrics with hierarchical-specific measures, we provided a thorough and nuanced evaluation of our models, ensuring that both node-level and full-path predictions were taken into consideration.
